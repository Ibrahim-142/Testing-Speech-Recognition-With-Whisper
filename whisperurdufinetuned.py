# -*- coding: utf-8 -*-
"""WhisperUrduFinetuned.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KKdsoOUoLhZbUnj_iz7DxH5GB0u0lHyW
"""

!pip install torch torchvision torchaudio
!pip install torchcodec
!pip install transformers datasets accelerate

from transformers import AutoProcessor, AutoModelForSpeechSeq2Seq
import torch

# Load custom Urdu Whisper model and processor
model_id = "Abdul145/whisper-medium-urdu-custom"

processor = AutoProcessor.from_pretrained(model_id)
model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)

device = "cuda" if torch.cuda.is_available() else "cpu"
model.to(device)

print(f"‚úÖ Model loaded on {device}")

from datasets import load_dataset, Audio

# Load the Urdu Common Voice processed dataset
ds = load_dataset("UmarRamzan/common-voice-urdu-processed")

# Resample audio to 16 kHz (required for Whisper models)
ds = ds.cast_column("audio", Audio(sampling_rate=16000))

print(ds)

def prepare_dataset(batch):
    audio = batch["audio"]
    batch["input_features"] = processor(
        audio["array"],
        sampling_rate=audio["sampling_rate"],
        return_tensors="pt"
    ).input_features[0]
    return batch

# Add input features column; remove unnecessary columns
ds = ds.map(prepare_dataset, remove_columns=["path", "variant"])
print("‚úÖ Audio features prepared successfully!")

from tqdm import tqdm
import torch

def transcribe(batch):
    # Convert list to tensor
    input_features = torch.tensor(batch["input_features"]).unsqueeze(0).to(device)
    with torch.no_grad():
        predicted_ids = model.generate(input_features)
    batch["prediction"] = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
    return batch

# Run inference on 5 samples from the test set
results = ds["test"].select(range(5)).map(transcribe)

for i, example in enumerate(results):
    print(f"\nüîä Example {i+1}")
    print(f"Reference : {example['sentence']}")
    print(f"Predicted : {example['prediction']}")

# Install dependencies
!pip install evaluate jiwer

import evaluate

# Load both metrics
wer_metric = evaluate.load("wer")
cer_metric = evaluate.load("cer")

# Collect predictions and references
preds = [x["prediction"] for x in results]
refs = [x["sentence"] for x in results]

# Compute metrics
wer_score = wer_metric.compute(predictions=preds, references=refs)
cer_score = cer_metric.compute(predictions=preds, references=refs)

# Display results
print(f"‚úÖ Word Error Rate (WER): {wer_score:.2%}")
print(f"‚úÖ Character Error Rate (CER): {cer_score:.2%}")

from tqdm import tqdm
import torch

# Batch size (adjust for GPU memory)
BATCH_SIZE = 4

def batched_transcribe(dataset, batch_size=BATCH_SIZE):
    preds, refs = [], []
    n = len(dataset)

    for i in tqdm(range(0, n, batch_size), desc="üöÄ Transcribing full test set"):
        batch = dataset.select(range(i, min(i + batch_size, n)))

        # Stack model input features into a batch tensor
        feature_tensors = [torch.tensor(f) for f in batch["input_features"]]
        features = torch.stack(feature_tensors).to(device)

        with torch.no_grad():
            predicted_ids = model.generate(features)

        # Decode predicted text
        transcriptions = processor.batch_decode(predicted_ids, skip_special_tokens=True)
        preds.extend(transcriptions)
        refs.extend(batch["sentence"])

    return preds, refs


# üïê Run transcription (this may take some time)
preds, refs = batched_transcribe(ds["test"], BATCH_SIZE)

print("‚úÖ Transcription completed.")
print(f"Total samples processed: {len(preds)}")

# Optional: Save results to avoid rerunning transcription
import pandas as pd
pd.DataFrame({"reference": refs, "prediction": preds}).to_csv("urdu_transcriptions.csv", index=False)
print("üíæ Transcriptions saved to 'urdu_transcriptions.csv'")

!pip install evaluate jiwer
import evaluate

# Load metrics
wer_metric = evaluate.load("wer")
cer_metric = evaluate.load("cer")

# Compute both metrics
wer_score = wer_metric.compute(predictions=preds, references=refs)
cer_score = cer_metric.compute(predictions=preds, references=refs)

print("\nüìä Evaluation Results on Full Test Set")
print(f"‚úÖ Word Error Rate (WER): {wer_score:.2%}")
print(f"‚úÖ Character Error Rate (CER): {cer_score:.2%}")

